// --------------------------------------------------------------------
// jsonl-helpers.civet â€“ Helpers for reading/writing JSON-Lines files and other fs-utils
// --------------------------------------------------------------------
fs from "fs-extra"
path from "node:path"
{ randomUUID } from "node:crypto"
glob from "fast-glob"

// Import logDebug - assuming it's exported from main or a utils file
// For simplicity, we'll assume it's available or we define a local one.
// In a real scenario, you'd refactor logDebug into a shared utils file.
logDebug := (...args: unknown[]) => {
    // Placeholder if not imported to avoid breaking helpers if used standalone.
    // In your app, logDebug from main.civet is available.
    console.log('[DEBUG]', ...args)
}

// Remove stray *.civetmantmp files that belong to our outputs
export cleanupTmpFiles := async (cwd: string) =>
    tmpFiles := await glob("**/*.civetmantmp", { cwd: cwd })
    for tmp of tmpFiles
        try
            await fs.unlink(tmp)
        catch (err: unknown)
            logDebug("[CLEANUP] Failed to remove stray tmp file", tmp, err)

// A record in our hashes.jsonl file
export type HashEntry = 
    file: string,
    sig: string,
    outFile: string

// ------------------------------------------------------------------
// Atomic write utility ensures we never leave partially-written files
// ------------------------------------------------------------------
export safeWrite := async (filePath: string, data: string | Buffer) =>
    await fs.ensureDir(path.dirname(filePath))
    tmp := filePath + "." + randomUUID() + ".civetmantmp"
    await fs.writeFile(tmp, data)
    await fs.rename(tmp, filePath)


// Reads a .jsonl file and reconstructs the latest state for each key.
// The last entry for any given file path wins.
export loadHashesFromJSONL := 
    async (filePath: string): Promise<Record<string, { sig: string, outFile: string }>> => 
        try
            unless await fs.pathExists(filePath) return {}
            content := await fs.readFile(filePath, "utf8")
            lines := content.split('\n')
            
            map: Record<string, { sig: string, outFile: string }> := {}

            for line of lines
                if !line.trim() continue
                try
                    entry := JSON.parse(line) as HashEntry
                    map[entry.file] = { sig: entry.sig, outFile: entry.outFile }
                catch (err: unknown)
                    logDebug("[JSONL] Skipping corrupted line in hashes.jsonl", { line, err })
            return map
        catch (err: unknown)
            logDebug("[JSONL] Failed to load hashes.jsonl, forcing full rebuild.", { filePath, err })
            return {}

// Overwrites the hashes file with the latest, complete state.
export writeHashesToJSONL :=
    async (filePath: string, hashes: Record<string, { sig: string, outFile: string }>) =>
        lines := Object.entries(hashes).map (([file, data]) => 
            JSON.stringify({ file, ...data })
        )
        content := lines.join('\n') + '\n'
        await safeWrite(filePath, content) 